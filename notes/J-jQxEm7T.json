{"_id":"note:J-jQxEm7T","title":"Optimizers","content":"## Optimizers\n\nWe have the gradients where we want them, now how do we apply them to optimize the weights? We use an optimizer!\n\n\n:::note Stocastic Gradient Descent (SGD)\nThe simplest optimizer. It simply takes the gradient and subracts it from the param:\n\n$$\n\\theta = \\theta - \\alpha\\frac{\\partial \\ell}{\\partial \\theta}\n$$\n\nWhere $\\ell$ is the loss and $\\alpha$ is the _learning rate_.\n:::\n\nSGD is good, a classic, and applied over batches. The main drawback is that it can get stuck in local minima very easily, with no momentum. It only depends on the loss of the current batch.\n\n:::note RMSProp\n\n\n:::\n\nRMSProp was developed by our boy Geoff Hinton. He never formally published it, but it's in public slides from a lecture of his. \n\n:::note Adam\n\n\n:::\n\n","tags":[],"folderPathname":"/ML","data":{},"createdAt":"2021-02-17T02:08:16.699Z","updatedAt":"2021-02-17T02:24:22.984Z","trashed":false,"_rev":"nr2RHLZQ2"}