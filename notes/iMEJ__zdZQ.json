{"_id":"note:iMEJ__zdZQ","title":"Activation Functions","content":"# Activation Functions\n\n## ReLU\nRectified Linear Unit\n$$ ReLU(x) = max(0, x) $$\nRelu is a classic, being applied to computer vision and then NLP as well.\nWhat makes ReLU good is that it creates **sparsity**.\n\n!!! caution The dead ReLU problem\nThere is a chance that the network is initialized such that parts of the  activation just before the ReLU are always negative. For example, if your input was always positive and your first weight was negative. Then, the result out of the ReLU would always be 0. But if it's always 0, it does not contribute to the loss function, meaning the gradient is 0. So the update is 0. So the weight never updates and the activation just sits there, wasting memory. You can run a test to see which activations are 0 throughout the whole dataset.\n!!!\n\n## GeLU\nGaussian Error Linear Units\n$$ GeLU(x) = max(0, x) $$\n\n## Swish\n[Paper](https://arxiv.org/pdf/1710.05941.pdf)\n\nSwish was found by Google using an optimization approach. It looks pretty similar to ReLU.\n$$y = x \\times sigmoid(x)$$\n:::danger tanh\n\n$$\n\\tanh(x) = \\frac{e^x-e^{-x}}{e^x + e^{-x}} = 2\\sigma(2x) -1\n$$\n\n:::\n\nThe important part about tanh is that it maps the whole real number line onto (-1, 1). This is good for keeping activations in a predictable range, but beware the vanishing gradient problem! tanh isn't used anymore, because of the vanishing gradient problem.\n\n:::danger Sigmoid\n\n$$\n\\sigma(x) = \\frac{1}{1 + e^x}\n$$\n\n:::\n\nSigmoid obviously maps all real values from 0 to 1. This is handy for interpretting things as probablilities. However, beware the vanishing gradient.\n\nSigmoid is also called the \"logistic function\".\n\n:::danger Softmax\n$$ \nSoftmax(\\vec{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K}{e^{z_j}}}\n$$\n\n:::","tags":[],"folderPathname":"/ML","data":{},"createdAt":"2020-10-22T22:18:39.194Z","updatedAt":"2021-02-17T03:08:42.861Z","trashed":false,"_rev":"Ya69lMrGp"}