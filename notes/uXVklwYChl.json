{"_id":"note:uXVklwYChl","title":"Activation Functions","content":"# Activation Functions\n\n## ReLU\nRectified Linear Unit\n$$ ReLU(x) = max(0, x) $$\nRelu is a classic, being applied to computer vision and then NLP as well.\nWhat makes ReLU good is that it creates **sparsity**.\n\n!!! caution The dead ReLU problem\nThere is a chance that the network is initialized such that parts of the  activation just before the ReLU are always negative. For example, if your input was always positive and your first weight was negative. Then, the result out of the ReLU would always be 0. But if it's always 0, it does not contribute to the loss function, meaning the gradient is 0. So the update is 0. So the weight never updates and the activation just sits there, wasting memory. You can run a test to see which activations are 0 throughout the whole dataset.\n!!!\n\n## GeLU\nGaussian Error Linear Units\n$$ GeLU(x) = max(0, x) $$\n\n## Swish\n[Paper](https://arxiv.org/pdf/1710.05941.pdf)\n\nSwish was found by Google using an optimization approach. It looks pretty similar to ReLU.\n$$y = x \\times sigmoid(x)$$\n## tanh\n\n\n## Sigmoid\n\n## Softmax","tags":[],"folderPathname":"/imported/ML","data":{},"createdAt":"2020-10-22T22:18:39.194Z","updatedAt":"2020-10-25T18:24:34.421Z","trashed":true,"_rev":"coi44F3dfK"}