{"_id":"note:8CiPaHtb3","title":"100 ML Questions","content":"This is a list of ML questions that I've been asked in interviews.\n\n1. Explain overfitting.\n\n:::danger Answer\nOverfitting is \n:::\n\n2. Why do we need non-linear activation functions?\n\n:::danger Answer\nAdding non-linear activation functions allows a neural network to learn more complicated functions. If we had no activation function, or perhaps just a linear activation function, then all the linear layers in the network could be represented\nas one linear layer. Imagine 3 fully connected layers, sequentially. They're just\nmatrix multiplication and vector addition. So if our activation function is linear,\nwe can collapse those 3 layers into just one layer.\n:::\n\n3. How do we prevent overfitting?\n\n* Stop training once the validation accuracy starts to drop\n* Stop training once the validation accuracy starts to drop\n\n\n4. Why do we need a validation set?\n\n:::danger Answer\nTo watch for overfitting, and ensure that our model performs well on data that it has not trained on, that it has not seen before.\n:::\n\n\n5. Explain the difference between a training, testing, and validation set.\n\n\n6. What is the vanishing gradient problem?\n\n:::danger Answer\nThe vanishing gradient problem is the problem where gradients in the network\nget so small that the updates to the parameters are negligible. The network effectively stops learning. This usually occurs when the derivative of some function in the network is close to 0.\n\n![](image-kl9htfvu.png)\n\nIn this tanh example, if our activations move out of the range [-3, 3], then they effectively pass no gradients back to previous layers. as a result, the weights before the tanh activation function hardly change throughout the training process, or they train very slowly.\n:::\n\n6. What is the dead ReLU problem?\n\n:::danger Answer\nObserve that the derivative of Relu on the negative real numbers is always 0. Thus,\nit does not pass a gradient to previous activations if the value of that previous activation was less than 0. If the weights are initialized in such a way that it always produces a value of 0, then that portion of the activation is never used to compute the output, and it's wasted computation/complexity.\n\nTo test:\nRun the entire test set through your network, watching the values of all your activations. If a value in any activation is 0 for all the instances in the test\nset (and it's proceeded by a ReLU), then it's affected by the dead ReLU problem.\nTo combat:\n* Use LeakyReLU, which never has the derivative equal to 0.\n* Initialize weights such that you rarely get an activation that is always 0.\n:::\n","tags":[],"folderPathname":"/ML","data":{},"createdAt":"2021-02-17T13:29:54.037Z","updatedAt":"2021-02-17T13:51:51.499Z","trashed":false,"_rev":"Ifhze69NG"}