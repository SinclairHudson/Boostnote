{"_id":"note:MvbSIf9qy2v","title":"Normalization","content":"# Normalization\n\nThis document does over different kinds of normalizations.\n\nThe core idea is to normalize the distributions of the activations of the networks, to make it easier for downstream layers to learn important features.\nYou can imagine that if the activation distribution differs wildly batch to batch, the network has trouble building consistent dependencies. This issue is called **covariate shift**.\nThe original Google batch normalization paper showed that models with batch norm converge in 7% of the time.","tags":[],"folderPathname":"/ML","data":{},"createdAt":"2020-10-22T22:15:59.298Z","updatedAt":"2020-10-22T22:18:34.936Z","trashed":false,"_rev":"OuhnVVY_7_I"}