{"title":"","content":"A problem with finite inputs isn't really interesting. We could pre-compute the desired output, put it in a hash, and then $O(1)$.\n\n![](image-kjvqcyjl.png)\n\nEvery model of Computation is wrong, but some are useful. LMAO this is getting over-used. :joy:.\n\n\n## Models of Computation\n\n1. Pseudo-code\n![](image-kjvsoowl.png)\n\nSo, a runtime might be $O(n)$ arithmetically, but keep in mind that an implementation can't represent infinitely large integers. There are overflows to worry about.\n\n![](image-kjvstcx7.png)\n\n2. Random Access Machine\n\n![](image-kjvsxeci.png)\n\nSo just assume that indexes and the like can fit into one machine word, constant time for all computations.\n\n3. Circuit Family -> goes all the way down to hardware :face_vomiting:\n4. Turing machine -> Time to access memory location $i$ is proportional to $i$.\n\nThe model is very important; it changes the running time of algorithms. Be careful which one is assumed, and what assumptions we're making. We will be using **word RAM** model.\n\n\n![](image-kjvt8n0x.png)\n\nWorst-case is the most important because we want a guarantee.\n\n![](image-kjvteigd.png)\nOf course, Big-O is **worst case upper bound**.\n\nHere are some properties:\n\n$$O(f(n)+g(n)) => O(max\\{f(n), g(n)\\})$$  \nThis rule is tricky. There could be $n$ where $f$ dominates, and some where $g$ dominates.\n\nBig O is also transitive, so \n$$f(n) \\in O(g(n)), g(n) \\in O(h(n)) \\implies f(n) \\in O(h(n))$$\n\n![](image-kjvtndal.png)\n\n\nTight bounds ($\\Theta$) are good for comparisons. We can't say that $O(n \\log n)$ is better than $O(n^2)$, because that's not specific enough.\n\n### good tidbits\n![](image-kjvts9ro.png)","tags":[],"folderPathname":"/CS341","data":{},"createdAt":"2021-01-13T18:00:47.290Z","updatedAt":"2021-01-13T19:37:19.236Z","trashed":false,"_id":"note:Lk5SQcfBk","_rev":"40-54890d67bd496a338b0d4f11b05e66b5"}