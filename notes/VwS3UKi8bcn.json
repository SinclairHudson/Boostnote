{"_id":"note:VwS3UKi8bcn","title":"Module 1:","content":"# Module 1:\n\nIntro terminology:\n\n!!! note Problem\nGiven a problem instance, carry out a particular computational task.\n!!!\n!!! note Problem Instance\nInput for the specified problem.\n!!!\n!!! note Problem Solution\nThe correct answer for a specified problem instance.\n!!!\n!!! note Size of a problem instance\nA positive integer which is a measure of the size of the instance $I$.\n!!!\n!!! note Algorithm\nA step-by-step process for carrying out a series of computations, given an arbitrary problem instance $I$. An algorithm solves a problem if for very instance $I$, the algorithm finds a valid solution for the instance $I$ in finite time.\n!!!\n\n!!! note Program\nAn implementation of an algorithm using a specified computer language.\n!!!\n\nThere are two metrics for assessing efficiency of a program: Runtime and Space. \"How long does it take to run?\" and \"How much memory does it need to run?\". These two are called **runtime efficiency** and **memory efficiency**. Or time complexity and memory complexity, or whatever. These terms get abused.\n\nWe could analyze efficiency empirically, but it would be tedious, hardware dependent, language dependent, implementation dependent. It would be stupid. So instead, we simplify algorithms and look at them mathematically. We don't talk about absolute time, but instead relative runtime, and the relationship between input size and runtime.\n\n!!! hint $O$-notation\n$f(n) \\in O(g(n))$ if $\\exists$ constants $c>0$ and $n_0 > 0$ such that $|f(n)| \\leq c|g(n)| \\forall n \\geq n_0$\nThis is essentially like an epsilon-delta limit. Note that it's not tight. $2n^2+3n+11 \\in O(n^2)$, but the same function is also $\\in O(n^{10})$. So if we want something tight, we've gotta use something else. Big O is like \"$f$ grows this fast or less fast\". Asymptotically not bigger.\n!!!\n!!! hint $\\Omega$-notation\n$f(n) \\in \\Omega(g(n))$ if $\\exists$ constants $c>0$ and $n_0 > 0$ such that $c|g(n)|  \\leq |f(n)|\\forall n \\geq n_0$\nThis is the opposite of Big O. Functions are members of $\\Omega(g(n))$ if $f$ grows asymptotically faster. $2n^2 \\in \\Omega(1), \\in \\Omega(n), \\in \\Omega(n^2)$. This one isn't really useful, but it's defined.\nAsymptotically not smaller.\n!!!\n!!! hint $\\Theta$-notation\n$f(n) \\in \\Theta(g(n))$ if $\\exists$ constants $c_1>0$, $c_2>0$ and $n_0 > 0$ such that $c_1|g(n)|  \\leq |f(n)| \\leq c_2|g(n)|\\forall n \\geq n_0$\nNow this one is spicy. It really says something. $2n^2+11n+3 \\in \\Theta(n^2)$\nImportantly, $f(n)\\in\\Theta(g(n)) \\iff f(n) \\in O(g(n)) \\cap \\Omega(g(n))$\nAsymptotically the same. The growth rates are the same.\n!!!\nThese are ok, but can we talk about strict inequalities?\n!!! hint $o$-notation\n$f(n) \\in o(g(n))$ if $\\exists$, **for all constants** $c>0$, an $n_0 > 0$ such that $|f(n)| < c|g(n)|\\forall n \\geq n_0$\nAsymptotically strictly smaller than $g(n)$. The growth rate of $f$ is smaller than the growth rate of $g$.\n!!!\n!!! hint $\\omega$-notation\n$f(n) \\in \\omega(g(n))$ if **for all constants** $c>0$, $\\exists$ constant  $n_0 > 0$ such that $0 \\leq c|g(n)|<|f(n)|\\forall n \\geq n_0$.\nAsymptotically strictly larger than $g(n)$. The growth rate of $f$ is larger than the growth rate of $g$.\nThis one is rarely proved from first principles.\n!!!\n\nRemember, all of the above are **sets**, families of functions.\n#### part 7\n![9e718e1d.png](9e718e1d-kl2sswa1.png)\n### Max Rules\n$O(f(n)+g(n)) = O(\\max{f(n), g(n)})$\n$\\Omega(f(n)+g(n)) = \\Omega(\\max{f(n), g(n)})$\n\nThere is also transitivity, which is trivial.\n\nWe can find membership of sets using a l'Hopital's rule limit.\n!!! note Finding complexity with limits\nSuppose $f(n)>0$ and $g(n) > 0$ for all $n \\geq n_0$. Suppose that\n$$L = \\lim_{n \\to \\infty}\\frac{f(n)}{g(n)}$$\nThen\n$$f(n) \\in \\begin{cases}\no(g(n)) & \\text{if } L=0 \\\\\n\\Theta(g(n)) & \\text{if } 0 < L < \\infty \\\\\n\\omega(g(n)) & \\text{if } L = \\infty \\\\\n\n\\end{cases}\n$$\nNote that this is a one way implication! You can't have membership and then assume that the limit exists!\n!!!\n\n#### part 8\nExample with limit rule.\n\n#### part 9\nExample where limit DNE. The solution is to bound the function on both sides, and then use the definition of $\\Theta(g(n))$ to prove membership.\n\n#### part 10\n$log(n) = log_2(n)$ in this course.\nCompare $log(n)$ and $n$ using the limit rule and l'hopital's. So $log(n) \\in o(n)$.\n$$ lim_{n \\to \\infty}\\frac{(log(n))^c}{n^d} = 0 $$\nwith recursive applications of L'Hopital's. So we find that $(log(n))^c \\in o(n^d)$, for arbitrary positive constants $c$ and $d$.\n\n#### part 11\n![c7bed1f1.png](c7bed1f1-kl2sswa1.png)\nSeeing what doubling the input size does to the runtime of these different growth rates.\n\n#### part 12\nNow we will get into actually using these techniques for real alg analysis.\n\n### Strategy 1:\n- **Elementary Operations** are constant time $\\Theta(1)$. Start by identifying these.\n- Next, look at the loops from inner to outer. These loops will be represented as summations of whatever is inside them.  \n- There will be a lot of nested summations. Work outwards. Keep track of all your indices!\n\nThis section goes over a simple nested loop example, it ends up being $\\Theta(n^2)$.\n#### part 13\nUsing the example of part 12, we look at solving the problem a different way. \n### Strategy 2:\nWe can find an upper and lower bound in our massive summation problem, by simplifying the problem. For example, you can add terms to make an upper bound, and remove some to make a lower bound. If you can show that the upper and lower bounds are in $O(g(n))$ and $\\Omega(g(n))$ respectively, then you can prove membership in $\\Theta(g(n))$.\n\n#### part 14\nAnother example, using tight $\\Theta$ bounds throughout first, and then using the other strategy of proving a $O$ bound and a $\\Omega$ bound separately.\n\n#### part 15\nOk, so far so good. But we haven't looked at algorithms that change based on input. The running time can change dependent on the input, and not just the size. For example, it takes almost no time to sort an array that's already sorted! The runtime can actually be a range of things, depending on the input.\n\n$T_A(I)$ Denotes the running time of alg $A$ on instance $I$.\n!!! hint Worst-case complexity of an alg\ntake the worst $I$.\n$$\nT_A(n) = max\\{{T_A(I):Size(I) = n}\\}\n$$\n!!!\n!!! hint Average-case complexity of an alg\naverage over all the $I$ of the same size.\n![343053c1.png](343053c1-kl2sswa0.png)\n!!!\n!!! hint Best-case complexity of an alg\ntake the best $I$\n!!!\nNote: We assume that all instances have the same probability of  occurence. You have to decide which kind of running time you're talking about, then you can discuss the membership of the time complexity in $O$ or $\\Omega$. In this course, **we default to worst case**. Proofs in this course don't always have to be super formal with a bunch of summations and everything kept in.\n\n#### part 16\nWe can't conclude that algs are more or less efficient than each other based on worst case time-complexity. Also, $O(n)$ is not a good notation. Use $\\Theta(n)$ bounds, because there's a chance we could prove a tighter $O$ bound. Remember, the average complexity is more important in real world. And remember, all these comparisons are in the limit of $n \\to \\infty$. With smaller sizes, constant terms become more significant.\n\n#### part 17 & 18\n\nAnalysis of mergesort.\n![f9e098ba.png](f9e098ba-kl2sswa0.png)\n\n![14833c75.png](14833c75-kl2sswa0.png)\n\n#### part 19\n![05615a12.png](05615a12-kl2sswa0.png)\n\n#### part 20\n![3d7b6acd.png](3d7b6acd-kl2sswa0.png)\nYou can use the following identities without proof:\n![6f037081.png](6f037081-kl2sswa0.png)","tags":[],"folderPathname":"/imported/CS240","data":{},"createdAt":"2020-05-20T21:35:50.542Z","updatedAt":"2020-06-01T20:00:50.258Z","trashed":true,"_rev":"NsJPb_DAAUU"}